To optimize the Deep Learning (ML) model accuracy when the Ontology misses, we should upgrade from the simple Centroid-based approach to a Logistic Regression Classifier.

Why?
Current (Centroids): Calculates the "average" of all Happy sentences and compares new sentences to that average. This is simple but can be inaccurate if sentences vary a lot.

Proposed (Logistic Regression): Learns the optimal boundary between Happy, Sad, and Angry in the 768-dimensional space of the LaBSE model. This is the standard industry approach for text classification with embeddings and is significantly more accurate.

The current centroids.pkl file is generated by the 
src/build_model.py
 script.

How to Generate it
Run this command in your terminal:

bash
python3 src/build_model.py
How it Works (The Logic)
Load Data: It reads your 
data/sinhala_samples.json
 (sentences) AND 
ontology/lexicon.json
 (keywords).
Vectorize: It uses the LaBSE model to convert every sentence and keyword into a mathematical vector (a list of 768 numbers).
Calculate Centroids:
It takes all vectors for "Happy" and calculates the Average (Mean) Vector.
It does the same for "Sad" and "Angry".
Save: It saves these 3 average vectors into data/centroids.pkl.